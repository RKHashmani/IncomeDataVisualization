@article{osti_421279,
title = {Scaling up the accuracy of Naive-Bayes classifiers: A decision-tree hybrid},
author = {Kohavi, R},
abstractNote = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propane a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers; the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently out-perform both constituents, especially in the larger databases tested.},
doi = {},
journal = {},
number = ,
volume = ,
place = {United States},
year = {1996},
month = {12}
}

@misc{RebeccaMerrett2019,
author = {{Rebecca Merrett}},
booktitle = {Data Science Dojo},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jan},
title = {{Census Income}},
url = {https://code.datasciencedojo.com/datasciencedojo/datasets/tree/master/Census Income},
urldate = {2020-11-04},
year = {2019}
}

@inproceedings{Bailey2002,
abstract = {Emerging Patterns are itemsets whose supports change significantly from one dataset to another. They are useful as a means of discovering distinctions inherently present amongst a collection of datasets and have been shown to be a powerful technique for constructing accurate classifiers. The task of finding such patterns is challenging though, and efficient techniques for their mining are needed. In this paper, we present a new mining method for a particular type of emerging pattern known as a jumping emerging pattern. The basis of our algorithm is the construction of trees, whose structure specifically targets the likely distribution of emerging patterns. The mining performance is typically around 5 times faster than earlier approaches. We then examine the problem of computing a useful subset of the possible emerging patterns. We show that such patterns can be mined even more efficiently (typically around 10 times faster), with little loss of precision. {\textcopyright} 2002 Springer-Verlag Berlin Heidelberg.},
author = {Bailey, James and Manoukian, Thomas and Ramamohanarao, Kotagiri},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/3-540-45681-3_4},
file = {:E$\backslash$:/References/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Bailey, Manoukian, Ramamohanarao - 2002 - Fast algorithms for mining emerging patterns.pdf:pdf},
isbn = {3540440372},
issn = {16113349},
mendeley-groups = {Courses/Statistical Data Analysis},
pages = {39--50},
publisher = {Springer Verlag},
title = {{Fast algorithms for mining emerging patterns}},
url = {https://link.springer.com/chapter/10.1007/3-540-45681-3{\_}4},
volume = {2431 LNAI},
year = {2002}
}
@phdthesis{Pelleg2004,
author = {Pelleg, Dan and Moore, Andrew},
school = {Carnegie Mellon University},
title = {{Scalable and Practical Probability Density Estimators for Scientific Anomaly Detection}},
url = {https://dl.acm.org/doi/book/10.5555/1023559},
year = {2004}
}

@article{Gionis2007,
abstract = {We consider the following problem: given a set of clusterings, find a single clustering that agrees as much as possible with the input clusterings. This problem, clustering aggregation, appears naturally in various contexts. For example, clustering categorical data is an instance of the clustering aggregation problem; each categorical attribute can be viewed as a clustering of the input rows where rows are grouped together if they take the same value on that attribute. Clustering aggregation can also be used as a metaclustering method to improve the robustness of clustering by combining the output of multiple algorithms. Furthermore, the problem formulation does not require a priori information about the number of clusters; it is naturally determined by the optimization function. In this article, we give a formal statement of the clustering aggregation problem, and we propose a number of algorithms. Our algorithms make use of the connection between clustering aggregation and the problem of correlation clustering. Although the problems we consider are NP-hard, for several of our methods, we provide theoretical guarantees on the quality of the solutions. Our work provides the best deterministic approximation algorithm for the variation of the correlation clustering problem we consider. We also show how sampling can be used to scale the algorithms for large datasets. We give an extensive empirical evaluation demonstrating the usefulness of the problem and of the solutions. {\textcopyright} 2007 ACM.},
author = {Gionis, Aristides and Mannila, Heikki and Tsaparas, Panayiotis},
doi = {10.1145/1217299.1217303},
file = {:E$\backslash$:/References/ACM Transactions on Knowledge Discovery from Data/Gionis, Mannila, Tsaparas - 2007 - Clustering aggregation.pdf:pdf},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
keywords = {Clustering aggregation,Clustering categorical data,Correlation clustering,Data clustering},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {mar},
number = {1},
pages = {4},
title = {{Clustering aggregation}},
url = {https://dl.acm.org/doi/10.1145/1217299.1217303},
volume = {1},
year = {2007}
}

@misc{AaronSchlegel2017,
author = {{Aaron Schlegel}},
booktitle = {AaronSchlegel.me},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jan},
title = {{Principal Component Analysis with R Example}},
url = {https://aaronschlegel.me/principal-component-analysis-r-example.html},
urldate = {2020-11-26},
year = {2017}
}
@misc{LukeHayden2018,
author = {{Luke Hayden}},
booktitle = {DataCamp},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {aug},
title = {{PCA Analysis in R - DataCamp}},
url = {https://www.datacamp.com/community/tutorials/pca-analysis-r},
urldate = {2020-11-26},
year = {2018}
}

@misc{GastonSanchez2013,
author = {{Gaston Sanchez}},
booktitle = {Visually Enforced},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jan},
title = {{7 Functions to do Metric Multidimensional Scaling in R | Visually Enforced}},
url = {http://www.gastonsanchez.com/visually-enforced/how-to/2013/01/23/MDS-in-R/},
urldate = {2020-11-26},
year = {2013}
}

@misc{Berkeley2011,
author = {Berkeley},
booktitle = {Department of Statistics, Berkeley},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {mar},
title = {{Cluster Analysis}},
url = {https://www.stat.berkeley.edu/{~}s133/Cluster2a.html},
urldate = {2020-12-06},
year = {2011}
}

@misc{ManishPathak2018,
author = {{Manish Pathak}},
booktitle = {DataCamp},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jul},
title = {{Hierarchical Clustering in R}},
url = {https://www.datacamp.com/community/tutorials/hierarchical-clustering-R},
urldate = {2020-12-06},
year = {2018}
}
@misc{PerceptiveAnalytics2017,
author = {{Perceptive Analytics}},
booktitle = {R-bloggers},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {dec},
title = {{How to Perform Hierarchical Clustering using R}},
url = {https://www.r-bloggers.com/2017/12/how-to-perform-hierarchical-clustering-using-r/},
urldate = {2020-12-06},
year = {2017}
}

@misc{BradleyBoehmke2017,
author = {{Bradley Boehmke}},
booktitle = {UC Business Analytics R Programming Guide},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {mar},
title = {{K-means Cluster Analysis}},
url = {https://uc-r.github.io/kmeans{\_}clustering},
urldate = {2020-12-06},
year = {2017}
}

@article{Belbin1992,
author = {Belbin, Lee and Faith, Daniel P. and Milligan, Glenn W.},
doi = {10.1207/s15327906mbr2703_6},
file = {:E$\backslash$:/References/Multivariate Behavioral Research/Belbin, Faith, Milligan - 1992 - A Comparison of Two Approaches to Beta-Flexible Clustering.pdf:pdf},
issn = {15327906},
journal = {Multivariate Behavioral Research},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jul},
number = {3},
pages = {417--433},
publisher = {Lawrence Erlbaum Associates, Inc.},
title = {{A Comparison of Two Approaches to Beta-Flexible Clustering}},
url = {https://www.tandfonline.com/doi/abs/10.1207/s15327906mbr2703{\_}6},
volume = {27},
year = {1992}
}

@misc{R-core2016,
author = {R-core},
booktitle = {R Documentation},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {nov},
title = {kmeans function},
url = {https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kmeans},
urldate = {2020-12-07},
year = {2016}
}

@misc{KasiaKulma2017,
author = {{Kasia Kulma}},
booktitle = {R-tastic},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {may},
title = {{Cluster Validation In Unsupervised Machine Learning}},
url = {https://kkulma.github.io/2017-05-10-cluster-validation-in-unsupervised-machine-learning/},
urldate = {2020-12-24},
year = {2017}
}
@misc{STHDA2015,
author = {STHDA},
booktitle = {Statistical Tools for High-throughput Data Analysis},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {oct},
title = {{Clustering Validation Statistics: 4 Vital Things Everyone Should Know}},
url = {http://www.sthda.com/english/wiki/wiki.php?id{\_}contents=7952{\#}external-clustering-validation},
urldate = {2020-12-24},
year = {2015}
}

@inproceedings{Meila2003,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering C to clustering C′. The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings.},
author = {Meila, Marina},
booktitle = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
doi = {10.1007/978-3-540-45167-9_14},
issn = {03029743},
keywords = {Clustering,Comparing partitions,Information theory,Measures of agreement,Mutual information},
mendeley-groups = {Courses/Statistical Data Analysis},
pages = {173--187},
publisher = {Springer, Berlin, Heidelberg},
title = {{Comparing clusterings by the variation of information}},
url = {https://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}14},
volume = {2777},
year = {2003}
}

@techreport{Brock2020,
author = {Brock, Guy and Pihur, Vasyl and Datta, Susmita and Datta, Somnath},
institution = {Department of Bioinformatics and Biostatistics, University of Louisville},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {jul},
title = {{clValid , an R package for cluster validation}},
year = {2020}
}

@misc{CosmaShalizi2011,
author = {{Cosma Shalizi}},
booktitle = {Carnegie Mellon University},
mendeley-groups = {Courses/Statistical Data Analysis},
month = {feb},
title = {{The Bootstrap}},
url = {https://www.stat.cmu.edu/{~}cshalizi/402/lectures/08-bootstrap/lecture-08.pdf},
urldate = {2020-12-24},
year = {2011}
}
